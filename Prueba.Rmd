---
title: "Proyecto 1: Analisis Exporatorio y Clustering"
author: "Stefan Quintana, Sofía Escobar, Jorge Caballeros, Wilfredo Gallegos"
date: "2023-03-24"
output: html_document
---

```{r,echo=FALSE}
library(dplyr)
library(FrF2)
library(magrittr)
library(ungroup)
library(dplyr)
library(tidyverse)
library(ggplot2)
library (MASS)
#librerias para el clustering
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor

datos <- read.csv("./defunciones_combinadas.csv")

```


# Análisis de las variables

Se tienen 6 variables

Edad <- Variable categórica\r\n

Causa.De.Muerte <- Variable cualitativa\r\n

Total <- Variable cuantitativa\r\n

Hombres <- Variable cuantitativa\r\n

Mujeres <- Variable cuantitativa\r\n

año_info <- Variable categórica\r\n


```{r}

summary(datos)
str(datos)

```

Ahora, se procede a observar si las variables cuantitativas respetan una distribución normal.

### Total
```{r}
library(nortest)
hist(datos$Total)
qqnorm(datos$Total)
qqline(datos$Total)
lillie.test(datos$Total)
```
Basándose en la gráfica, donde se puede notar que los puntos se alejan de la recta teórica, podemos argumentar que los datos no respetan una distribución normal, lo cuál se confirma en el test Lillie, donde se rechaza la hipótesis nula, por lo tanto, se concluye que los datos no provienen de una distribución normal.

### Hombres
```{r}
hist(datos$Hombres)
qqnorm(datos$Hombres)
qqline(datos$Hombres)
lillie.test(datos$Hombres)
```
Al igual que con los datos Totales, las muertes de los hombres no respetan una distribución normal.

### Mujeres
```{r}
hist(datos$Mujeres)
qqnorm(datos$Mujeres)
qqline(datos$Mujeres)
lillie.test(datos$Mujeres)
```
Al igual que con los datos Totales, las muertes de los hombres no respetan una distribución normal.

Ahora, se crean bases de datos agrupadas por causas de muerte y por año
```{r}
datos_causa <- datos %>% group_by(Causa.de.muerte) %>% summarize(HombresTot = sum(Hombres), MujeresTot = sum(Mujeres), Tot = sum(Total))

datos_anual <- datos %>% group_by(año_info) %>% summarize(HombresTot = sum(Hombres), MujeresTot = sum(Mujeres), Tot = sum(Total))
```

Se realiza el análisis de distribución a las mismas variables con las nuevas bases de datos:

## Causa

### Hombres
```{r}
hist(datos_causa$HombresTot)
qqnorm(datos_causa$HombresTot)
qqline(datos_causa$HombresTot)
lillie.test(datos_causa$HombresTot)
```
Se revela con el test y con las gráficas que las muertes de los hombres no respetan una distribución normal si se agrupan por causa.


### Mujeres
```{r}
hist(datos_causa$MujeresTot)
qqnorm(datos_causa$MujeresTot)
qqline(datos_causa$MujeresTot)
lillie.test(datos_causa$MujeresTot)
```
Se revela con el test y con las gráficas que los decesis de las mujeres no respetan una distribución normal si se agrupan por causa.

### Total
```{r}
hist(datos_causa$Tot)
qqnorm(datos_causa$Tot)
qqline(datos_causa$Tot)
lillie.test(datos_causa$Tot)
```
Al igual que con los datos pasados el total de decesos no respeta una distribución normal si son agrupadas por causa.

## Año

### Hombres
```{r}
hist(datos_anual$HombresTot)
qqnorm(datos_anual$HombresTot)
qqline(datos_anual$HombresTot)
lillie.test(datos_anual$HombresTot)
```
Ya que el valor-p en el test de Lillie es mayor a 0.05 no se puede concluir que los decesos de los hombres no respetan una distribución normal si se agrupan por año.

### Mujeres
```{r}
hist(datos_anual$MujeresTot)
qqnorm(datos_anual$MujeresTot)
qqline(datos_anual$MujeresTot)
lillie.test(datos_anual$MujeresTot)

```
Al igual que con los decesos de los hombres, no se puede concluir que las muertes de las mujeres no respetan una distribución normal.

### Total
```{r}
hist(datos_anual$Tot)
qqnorm(datos_anual$Tot)
qqline(datos_anual$Tot)
lillie.test(datos_anual$Tot)
```

Con un valor-p mayor a 0.05 no se puede concluir que el Total de decesos no se ajusta a una distribución normal si se agrupan por año.


Algunas preguntas interesantes que pueden surgir son:\r\n

* ¿Cuál es la causa de muerte más común en el dataset?


* ¿Cómo ha cambiado la tasa de mortalidad a lo largo de los años en el dataset?


* ¿Existen patrones estacionales en las causas de muerte?


Discutiendo se pudo llegar a la siguiente incognita en específico la cual es nuestro tema de investigación para esta base de datos. 

**Evolucionón de las causas de muerte en Guatemala durante el periodo de 2009 - 2019** 

Para poder responder la pregunta primero debemos de realizar gráficos del comportamiento de cada causa de muerte en cada año.
```{r}
library(dplyr)

datos_filtrados <- datos %>%
  group_by(año_info, Causa.de.muerte) %>%
  filter(n() == max(n())) %>%
  ungroup()

tabla_frecuencia <- table(datos_filtrados$Causa.de.muerte, datos_filtrados$año_info)
tabla_frecuencia <- as.data.frame(tabla_frecuencia)


library(knitr)
kable(tabla_frecuencia, caption = "Tabla de frecuencia de Causa de muerte y años")

```

Ahora graficamos la tabla por cada causa de muerte:
```{r}


# Ciclo para crear un histograma por cada causa de muerte
for (i in unique(tabla_frecuencia$Var1)) {
  
  # Filtrar los datos por causa de muerte
  datos_causa_muerte <- filter(tabla_frecuencia, Var1 == i)
  
  # Crear el gráfico de histograma
  grafico <- ggplot(data = datos_causa_muerte, aes(x = Var2, y = Freq)) +
    geom_bar(stat = "identity", fill = "blue") +
    ggtitle(paste0("Frecuencia de ", i, " por año")) +
    xlab("Año") + ylab("Frecuencia") +
    theme(plot.title = element_text(size = 20, face = "bold"),
          axis.title.x = element_text(size = 16),
          axis.title.y = element_text(size = 16),
          axis.text.x = element_text(size = 14),
          axis.text.y = element_text(size = 14))
  
  # Imprimir el gráfico
  print(grafico)
}

# Cargamos las librerías dplyr y tidyr
library(dplyr)
library(tidyr)

# Filtramos la tabla para excluir los valores de Var1 que se llamen "Todas las causas" y "Otras causas"
tabla_filtrada <- tabla_frecuencia %>% filter(Var1 != "Todas las causas" & Var1 != "Otras causas" & Var1 != "Síntomas, signos y hallazgos anormales clínicos y de laboratorio, no clasificados en otra parte" )

# Agrupamos la tabla por año y calculamos la causa de muerte con mayor frecuencia para cada año
tabla_resumen <- tabla_filtrada %>% 
  group_by(Var2) %>% 
  summarise(principal_causa = Var1[which.max(Freq)])

# Generamos la nueva tabla con la forma solicitada
tabla_final <- data.frame(Año = tabla_resumen$Var2, `Principal Causa de Muerte` = tabla_resumen$principal_causa)

tabla_final
```



Los resultados alojan información valiosa de las variables:

* Causa de Muerte
* Año de Muerte

Podemos observar que estas variables nos pueden ayudar para resolver el la problemática planteada.


Podemos observar en las gráficas anteriores que hay causas de muerte en las cuales su frecuencia de ocurrecia es en un año en particular, y muchos de estos son frecuencias de baja incidencia, tal sería el caso por ejemplo de la muerte causada por afecciones respiratorias(gráfica 3), que tuvo una ocurrencia en el año 2015, también el de ahorcamientos o estrangulamientos(gráfica 10) con una ocurrencia de un evento por año en los años 2009 y 2010, la frecuencia de muerte por amiloidosis(gráfica 11) de 3 veces en el año 2015, la frecuencia de apendicitis aguda(gráfica 13) en el año 2017, la frecuencia de muerte por Ascariasis(gráfica 14) de 1 vez en el año 2012 y asi entre muchas, de las cuales 31 de todas las causas tienen incidencia en 2 o menos años.




# Principales causas de muerte por año


##Clustering 
Para poder hacer un clustering efectivo debemos de tener todas nuestras variables de forma numerica y normalizadas.
```{r hopkins} 
##primero debemos de volver numericas nuestras variables y finalmente escalarlas

Causa.de.muerte <- as.numeric(tabla_frecuencia[,"Var1"])
Total <- as.numeric(tabla_frecuencia[,"Freq"])
año_info <- as.numeric(tabla_frecuencia[,"Var2"])

datosc <- data.frame(Causa.de.muerte,año_info,Total)

#Escalar los datos
datosCS <- scale(na.omit(datosc))
```

Luego de la normalizacion de los datos obtenemos un estadistico de Hopkins para determinar el agrupamiento puede ser factible.

##Estadistico de hopkins
```{r}
hopkins(datosCS)
datos_dist<- dist(datosCS)

```
El valor del estadístico de hopkins está alejado de 0.5 por lo que los datos no son aleatorios hay altas posibilidades de que sea factible el agrupamiento.  

```{r}
#Matriz de distancia
datos_dist<- dist(datosCS)
fviz_dist(datos_dist, show_labels = F)
```

Como se puede observar en la VAT sí se observan ciertos patrones por lo que se ratifica lo que arroja el estadístico de hopkings.  

Al determinar que si es posible realizar un agrupamiento, determinamos de dos maneras cuantas agripaciones debeiamos de utilizar.

####Cantidad de grupo que debemos de hacer
```{r metodo de codo factoextra}

fviz_nbclust(datosCS, kmeans, method = "wss") +
labs(subtitle = "Metodo del codo")
```

```{r silueta factoextra}
fviz_nbclust(datosCS, kmeans, method = "silhouette") +
labs(subtitle = "Método de la silueta")

```

Luego de analizar los algoritmos se determinó que el numero de agrupaciones debía de ser de 5, ya que segun el método del codo y el método del a silueta, esta cantidad es la más óptima. Luego de este análisis procedemos a  agrupar por medio de K-medias

Agrupamos por medio de 2 distintos tipos algoritmos de agrupación:

### K-Medias  
```{r}
km<-kmeans(datosCS,5,iter.max =100)
plotcluster(datosCS,km$cluster) 

```

Como se observa en la imagen, el primer paso es escoger el numero de grupos K, en este caso fue 5 tal como se justificó anteriormente, posterior a ello se establecen k centroides en el espacio de datos.

```{r}
fviz_cluster(km, data = datosCS,geom = "point", ellipse.type = "norm")
```

```{r}
silkm<-silhouette(km$cluster,dist(datosCS))
mean(silkm[,3]) 
Kmean<-mean(silkm[,3]) 
```

A partir de los 2 gráficos anteriores, podemos observar que los datos separados en 5 grupos se presentan juntos entre si, podemos observar que la intersección entre cada uno de los 5 grupos es mínima, lo cual puede ser un buen indicador para nuestro agrupamiento. Esto puede ser evaluado de mejor manera a traves de un Cluster jerárquico.

#### Cluster jerárquico

```{r}
matriz_dist<- dist(datosCS)
hc<-hclust(datos_dist, method = "ward.D2") #Genera el clustering jerarquico de los datos
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
cutree(hc, h = 5)
rect.hclust(hc,k=5)
groups<-cutree(hc,k=5) #corta el dendograma, determinando el grupo de cada fila
```


```{r}
silhc<-silhouette(groups,datos_dist)
mean(silhc[,3]) 
Jerarquico<-mean(silhc[,3]) 
```

Como se observa la silueta del algoritmo cluster jerárquico fue de `r mean(silhc[,3])`, indicando que la agrupación no es la mejor.

El gráfico de la silueta de K-means sería el siguiente:
```{r}
plot(silkm, cex.names=.4, col=1:4, border=NA)
```


Y el gráfico de la silueta de Jerargico sería: 
```{r}
plot(silhc, cex.names=.4, col=1:4, border = NA)
```

Al recopilar el promedio de los datos obtenemos:
```{r}
df <- data.frame(Algoritmo=c("K-mean", "Jerarquico"),
                Silueta=c(Kmean, Jerarquico))
df
```
Se puede observar que el cluster jerárquico fue el que obtuvo el mejor resultado en la prueba de silueta pero que el algorito Kmeans es mayor que el Jerarguico por 0.5 puntos. Al observar los valores de las siluetas observamos que no es la mejor agrupacion pero que no es mala de igua forma. Tambien observamos que en los grupos de ambas siluetas hay elementos mal ubicados ya que hay valores negativos dentro de las siluetas.

```


